#!/usr/bin/env bash

source message_functions || exit 1

PARSE_SCRIPT="check_bclconvert_conversion.pl"
ROUNDING_SCRIPT="base_count_to_gbase.py"

CONVERSION_REPORT="hmf_conversion_report.tsv"
CONVERSION_REPORT_ERR="hmf_conversion_report.err"
SAMPLES_REPORT="hmf_samples_report.tsv"
RUN_REPORT="hmf_run_report.tsv"

FORENSICS_BUCKET="bcl-forensics-prod-1"
NAS_REPORT_PATH="/volume1/reports/bclconvert"
NAS_WEB_REPORT_PATH="/volume1/web/qc/conversion"

command -v "$PARSE_SCRIPT" >/dev/null || die "Dependency not found: $PARSE_SCRIPT"
command -v "$ROUNDING_SCRIPT" >/dev/null || die "Dependency not found: $ROUNDING_SCRIPT"
command -v "hmf_api_get" >/dev/null || die "Dependency not found: hmf_api_get"
command -v "sync_dir_to_nas" >/dev/null || die "Dependency not found: sync_dir_to_nas"

error_log="output/results/Logs/Errors.log"
samplesheet_csv="input/SampleSheet.csv"
runinfo_xml="input/RunInfo.xml"
demux_csv="output/results/Reports/Demultiplex_Stats.csv"
quality_csv="output/results/Reports/Quality_Metrics.csv"
top_unknown_csv="output/results/Reports/Top_Unknown_Barcodes.csv"
required_files=("$error_log" "$samplesheet_csv" "$runinfo_xml" "$demux_csv" "$quality_csv" "$top_unknown_csv")

[[ "$(hostname)" =~ ^ops-vm-prod ]] || die "This script is meant to be executed from ops-vm only!"

if [[ ! $1 || $1 == "-h" || $1 == "--help" ]]; then
    script=$(basename "$0")
    echo ""
    echo " Descr: downloads flowcell conversion forensic files and performs QC"
    echo " Usage: $script <flowcell-id> [<flowcell-id-2> <flowcell-id-N>]"
    echo "    Eg: $script AHCFL5CCXY BHCCVKCCXY"
    echo ""
    exit 1
fi
flowcells_ids=("$@")

function main() {
    script="$(basename $0)"
    tmpdir="/tmp/${script}/$(date '+%y%m%d%M%S')"
    info "Started ${script} with tmp dir [${tmpdir}]"
    mkdir -p "${tmpdir}" || die "Unable to create tmp dir [$info]"

    for flowcell_id in "${flowcells_ids[@]}"; do

        info="$flowcell_id"
        info "Start processing flowcell [${info}]"

        flowcell_forensics_url=$(get_forensics_url "${FORENSICS_BUCKET}" "${flowcell_id}") || die "Unable to find forensics [$info]"
        flowcell_api_info=$(get_flowcell_from_api "${flowcell_id}") || die "Unable to find flowcell in API [$info]"
        api_status=$(jq -r '.status' <<< "$flowcell_api_info")
        api_name=$(jq -r '.name' <<< "$flowcell_api_info")
        info="${info},${api_name},${api_status}"

        [[ "$api_status" == "Converted" || "$api_status" == "Failed" ]] || die "Unexpected status [${api_status}] in API [$info]"

        seq_run=$(basename "${flowcell_forensics_url}")
        outdir="${tmpdir}/${seq_run}"

        conversion_report_tsv="${outdir}/${CONVERSION_REPORT}"
        conversion_report_err="${outdir}/${CONVERSION_REPORT_ERR}"
        samples_report_tsv="${outdir}/${SAMPLES_REPORT}"
        run_report_tsv="${outdir}/${RUN_REPORT}"

        mkdir -p "${outdir}" || die "Unable to create output dir [$info]"
        info "  Output dir created [${outdir}]"

        info "  Retrieving files from bucket [$info]"
        for file in "${required_files[@]}"; do
            file_url="${flowcell_forensics_url}/${file}"
            gsutil -q cp -r "${file_url}" "${outdir}" || die "Unable to copy file [$file_url]"
        done

        hmf_name="$(grep '^Experiment' "${outdir}/SampleSheet.csv" | cut -d',' -f2 | tr -d '\r')"
        nas_dir_name="${hmf_name}__${seq_run}"
        nas_dir_path="${NAS_REPORT_PATH}/${nas_dir_name}"
        nas_web_dir_path="${NAS_WEB_REPORT_PATH}/${nas_dir_name}"

        [[ -s "${outdir}/Errors.log" ]] && warn "  Non-empty error log!!"

        info "  Creating conversion report [$info]"
        ${PARSE_SCRIPT} -sampleSheet "${outdir}/SampleSheet.csv" -runInfoXml "${outdir}/RunInfo.xml" \
          -qualityCsv "${outdir}/Quality_Metrics.csv" -demuxCsv "${outdir}/Demultiplex_Stats.csv" \
        > "${conversion_report_tsv}" 2> "${conversion_report_err}" || die "Script $PARSE_SCRIPT exited with non zero exit status"

        if [[ -s "${conversion_report_err}" ]]; then
            warn "  Non-empty error log please check [${conversion_report_err}]"
        fi

        info "  Processing conversion report [$info]"
        grep "^## RunOverview" "${conversion_report_tsv}" | sed 's#\#\#\ RunOverview INFO:\ ##g' > "${run_report_tsv}"
        process_samples "${conversion_report_tsv}" > "${samples_report_tsv}"

        info "  Syncing output files to NAS [${nas_dir_path}]"
        sync_dir_to_nas "${outdir}" "${nas_dir_path}"

        info "  Syncing output files to NAS web [${nas_web_dir_path}]"
        sync_dir_to_nas "${outdir}" "${nas_web_dir_path}"
    done

    info "SAMPLE info"
    find "$tmpdir" -name "${SAMPLES_REPORT}" -exec cat {} \; | sort -r
    echo ""

    info "RUN info"
    find "$tmpdir" -name "${RUN_REPORT}" -exec cat {} \; | sort -r
    echo ""

    info "Extra actions:"
    echo "rm -r ${tmpdir}"
    echo ""
}

function get_flowcell_from_api () {
    local flowcell_id=$1 && shift
    local json=""
    local count=0
    json=$(hmf_api_get "flowcells?flowcell_id=${flowcell_id}")
    count=$(jq 'length' <<< "$json")
    if [[ "$count" -eq 1 ]]; then
        flowcell_json=$(jq '.[-1]' <<< "$json")
        echo "$flowcell_json" && return 0
    else
        return 1
    fi
}

function get_forensics_url () {
    local bucket=$1 && shift
    local fcid=$1 && shift
    url=$(gsutil ls -d "gs://${bucket}/*${fcid}")
    url=${url%/}
    echo "$url"
}

function process_samples () {
    local tsv=$1 && shift
    grep ^SAMPLE "$tsv" | while read -r line; do
        yld_seq=$(echo "${line}" | cut -f3)
        yld_seq_base_count=$((yld_seq*1000000))
        q30_seq=$(echo "${line}" | cut -f4)
        sample_id=$(echo "${line}" | cut -f8)
        sample_nm=$(echo "${line}" | cut -f9)
        submission=$(echo "${line}" | cut -f10)
        #reporting_id="NA"
        api_samples_json=$(hmf_api_get "samples?barcode=${sample_id}")
        if [[ "${api_samples_json}" == "[]" ]]; then
            smp_status="Unregistered"
            yld=""
            req=""
        else
            sample=$(jq -cr '.[-1]' <<< "${api_samples_json}")
            sample_tsv=$(echo "${sample}" | jq -r '[.status,.yld,.yld_req] | @tsv')
            #sample_tsv=$(echo "${sample}" | jq -r '[.status,.yld,.yld_req,.reporting_id] | @tsv')
            # Note: last cut shortens "Insufficient Quality" to "Insufficient"
            smp_status=$(echo "${sample_tsv}" | cut -f1 | cut -d" " -f1)
            yld=$(echo "${sample_tsv}" | cut -f2)
            req=$(echo "${sample_tsv}" | cut -f3)
            #reporting_id=$(echo "${sample_tsv}" | cut -f4)
            if [[ "${yld}" == "" && "${req}" != ""  ]]; then
                # This happens when no FASTQ files are registered yet, but the sample is registered properly
                yld="0"
            fi
        fi

        if [[ "${req}" == "" ]]; then
          # No required yield known
          req_gbase="?"
        elif [[ ${req} -eq 1 ]]; then
          # A req of 1 base is used when there is no req, but 0 would cause the sample to incorrectly be 'Validated' before being sequenced
          req_gbase="0"
        else
          req_gbase=$("${ROUNDING_SCRIPT}" --base_count "${req}") || die "Could not convert req for ${sample_id} to gbase: ${req}"
        fi

        if [[ "${yld}" == "" ]]; then
          yld_gbase="?"
        elif [[ "${req_gbase}" == "?" ]]; then
          yld_gbase=$("${ROUNDING_SCRIPT}" --base_count "${yld}") || \
                die "Could not convert yld for ${sample_id} to gbase: ${yld}"
        else
          yld_gbase=$("${ROUNDING_SCRIPT}" --base_count "${yld}" --round_like "${req_gbase}") || \
                die "Could not convert yld for ${sample_id} to gbase rounded like ${req_gbase}: ${yld}"
        fi

        if [[ "${req_gbase}" == "?" ]]; then
          yld_seq_gbase=$("${ROUNDING_SCRIPT}" --base_count "${yld_seq_base_count}") || \
                die "Could not convert yld_seq_base_count for ${sample_id} to gbase: ${yld}"
        else
          yld_seq_gbase=$("${ROUNDING_SCRIPT}" --base_count "${yld_seq_base_count}" --round_like "${req_gbase}") || \
                die "Could not convert yld_seq_base_count for ${sample_id} to gbase rounded like ${req_gbase}: ${yld}"
        fi
        # Note: Samples-for-database have two runs (diagnostic and research). We can ignore the research one.
        # Note: Only specific inis: 2=CPCT.ini 4=SingleSample.ini 6=Somatic.ini 8=KG.ini 10=Fastq.ini 38=ShallowSeq.ini 51=Rna.ini
        api_run=$(
          hmf_api_get "runs?barcode=${sample_id}" | \
          jq '[.[] | select(.bucket//"NA"|test("research-pipeline")|not)]' | \
          jq '[.[] | select(.ini_id==2 or .ini_id==4 or .ini_id==6 or .ini_id==8 or .ini_id==10 or .ini_id==38 or .ini_id==51 or .ini_id==55)]' | \
          jq -r '.[] | [.status,.ini,.set.name] | @tsv' | \
          tail -1
        )

        if [[ "${api_samples_json}" == "[]" ]]; then
            status_info="q=${q30_seq} y=${yld_seq_gbase} (no sample found in API by barcode!)"
        elif [[ "${req_gbase}" == "?" ]]; then
            status_info="q=${q30_seq} y=${yld_seq_gbase}+${yld_gbase}/${req_gbase} ${smp_status} (sample found in API but no yield requirement!)"
        elif [[ "${api_run}" == "" ]]; then
            status_info="q=${q30_seq} y=${yld_seq_gbase}+${yld_gbase}/${req_gbase} ${smp_status} (sample found in API but no run!)"
        else
            run_status=$(cut -f1 <<< "${api_run}")
            ini=$(cut -f2 <<< "${api_run}")
            ini=${ini/.ini/}
            set_name=$(cut -f3 <<< "${api_run}")
            status_info="q=${q30_seq} y=${yld_seq_gbase}+${yld_gbase}/${req_gbase} ${smp_status}|${run_status}|${ini}"
        fi
        printf "%s\t%s\t%s\t%s\t%s\t%s\n" "${submission}" "${sample_id}" "${sample_nm}" "${ini}" "${status_info}" "${set_name}"
        #printf "%s\t%s\t%s\t%s\t%s\t%s\t%s\n" "${submission}" "${sample_id}" "${sample_nm}" "${reporting_id}" "${ini}" "${status_info}" "${set_name}"
   done | sort -k1,1 -k3,3
}

main
