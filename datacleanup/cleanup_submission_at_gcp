#!/usr/bin/env bash

source message_functions || exit 1

OUT_ROOT="/data/ops/recurrent/submission_cleanup/logs"
GSUTIL_CMD_PARAMS="-m -o GSUtil:parallel_process_count=7 -o GSUtil:parallel_thread_count=1"

GCP_PROJECT="hmf-ops"
GCP_USER="hmf-ops"
GCP_SERVICE_ACCOUNT="${GCP_USER}@${GCP_PROJECT}.iam.gserviceaccount.com"
do_execute_cleanup=false

print_usage(){
    echo "-----"
    echo " Descr: Searches for data in GCP buckets and print code to cleanup"
    echo " Usage: $(basename $0) -s \$submission"
    echo "        $(basename $0) -s \$submission -e"
    echo " Examp: nohup $(basename $0) -s HMFregXXXX > \$HOME/logs/tmp.log &"
    echo " Notes: 1) Writes all log files to $OUT_ROOT"
    echo "        2) CAUTION with param -e (this automatically executes deletion)"
    echo "-----"
    exit 1
}

while getopts ':s:e' flag; do
    case "$flag" in
        s) submission=${OPTARG} ;;
        e) do_execute_cleanup=TRUE ;;
        *) print_usage
        exit 1 ;;
    esac
done

if [[ -z "$submission" ]]; then
    print_usage
fi

main() {
    info "Starting with $(basename $0) ($submission)"

    out_path="$OUT_ROOT/$submission"
    if [[ -d "$out_path" ]]; then
        YYMMDD=$(date '+%y%m%d')
        info "  Output path exists ($out_path)"
        out_path="${OUT_ROOT}/${submission}/${YYMMDD}"
        info "  Output path reset to include date ($out_path)"
    fi

    # sanity checks
    info "  Performing various sanity checks"
    [[ -w "$OUT_ROOT" ]] || die "Out root not writable ($OUT_ROOT)"
    [[ ! -d "$out_path" ]] || die "Output directory already exists ($out_path)"
    [[ ! -f "$log_file" ]] || die "Log file already exists ($log_file)"
    [[ ! -f "$rm_cmd_file" ]] || die "Removal cmd file already exists ($rm_cmd_file)"
    [[ ! -f "$rm_log_file" ]] || die "Removal log already exists ($rm_log_file)"

    # setup output dir
    mkdir -p "$out_path" || die "Unable to create output dir ($out_path)"
    log_file="$out_path/setup.log"
    rm_cmd_file="$out_path/cleanup_at_gcp"
    rm_log_file="$out_path/cleanup_at_gcp.log"

    # also write startup msg to log file for tracking when using parallel execution
    info "Starting with $(basename $0) ($submission)" >> "$log_file"

    info "Switching to GCP service account $GCP_SERVICE_ACCOUNT" >> "$log_file"
    msg=$(gcloud config set account "$GCP_SERVICE_ACCOUNT" 2>&1)
    info "  $msg" >> "$log_file"

    info "Retrieving samples by submission" >> "$log_file"
    samples_json=$(hmf_api_get "samples?submission=$submission")
    sample_ids=$(jq -r '. | map(.id) | @csv' <<< "${samples_json}")
    info "Samples found: ${sample_ids}" >> "$log_file"

    # add start message to final cleanup cmd file
    echo "echo '[INFO] Starting cleanup for submission $submission'" >> "$rm_cmd_file"

    processed_ids=""
    while read -r sample; do
        sample_id=$(echo "$sample" | cut -f1)
        sample_status=$(echo "$sample" | cut -f2)
        info "Processing sample (id=$sample_id)" >> "$log_file"
        process_sample "$sample_id" "$rm_cmd_file" "$log_file"
        if [[ "${sample_status}" != "Unregistered" ]]; then
            # Registered samples are always linked to sets/runs
            info "Processing runs for sample (id=$sample_id)" >> "$log_file"
            processed_ids=$(process_runs_for_sample "$sample_id" "$rm_cmd_file" "$log_file" "$processed_ids")
        fi
    done < <(echo "$samples_json" | jq -cr '.[] | [.id,.status] | @tsv')
    echo "echo '[INFO] Finished cleanup for submission $submission'" >> "$rm_cmd_file"

    info "Making cleanup cmd file executable ($rm_cmd_file)" >> "$log_file"
    chmod +x "$rm_cmd_file"

    run_count=$(echo "$processed_ids" | tr " " "\n" | grep -c "ID:")
    info "A total of $run_count runs were processed" >> "$log_file"
    if [[ "$do_execute_cleanup" == TRUE ]]; then
        info "Configured for automatic cleanup so executing cleanup file ($rm_cmd_file)" >> "$log_file"
        rm_err_file="${rm_log_file}.err"
        $rm_cmd_file >"$rm_log_file" 2>"$rm_err_file"
    else
        info "You can now execute deletion with:" >> "$log_file"
        info "  nohup $rm_cmd_file > $rm_log_file &" >> "$log_file"
        info "  Setup is complete, check log file for further actions: $log_file"
    fi
    info "Finished with $(basename $0) ($submission)" >> "$log_file"
    info "Finished with $(basename $0) ($submission)"
}

process_runs_for_sample() {
    local sample_id=$1 && shift
    local cmd_file=$1 && shift
    local log_file=$1 && shift
    local processed_ids=$1 && shift
    sets_json=$(hmf_api_get "sets?sample_id=$sample_id")

    while read -r set; do
        set_id=$(echo "$set" | cut -f1)

        runs_json=$(hmf_api_get "runs?set_id=$set_id")
        while read -r run; do
            run_id=$(echo "$run" | cut -f1)
            run_name=$(echo "$run" | cut -f2)
            bucket=$(echo "$run" | cut -f3)
            bucket=${bucket/_/-} # fix for old API records
            run_status=$(echo "$run" | cut -f4)
            ini_name=$(echo "$run" | cut -f5)

            id_present=$(echo "$processed_ids" | grep -c "ID:$run_id ")
            if [[ "$id_present" -gt 0 ]]; then
                info "  Not processing run $run_id because already processed ($run_name)" >> "$log_file"
                continue
            fi

            # Really ugly way to keep track of IDs that we already visited (Somatic runs have two samples)
            # Bash only supports references from v4.3 so need to work with expanding string
            processed_ids+="ID:$run_id " # the space is important!

            gs_url="gs://$bucket/$run_name"
            run_type=$(run_type_by_ini_name "$ini_name")

            run_info="$run_id ($run_status) of type $run_type for set $set_id by sample $sample_id ($sample_status)"
            info "  Processing run $run_info" >> "$log_file"
            echo "echo '[INFO] Executing steps for run $run_info'" >> "$rm_cmd_file"

            if [[ "$run_type" == "fastq" ]]; then
                info "  SKIPPING: run is type fastq so cleanup done at sample level for $run_info" >> "$log_file"
            elif [[ "$run_type" == "pipeline" ]]; then
                process_gs_url "$run_status" "$gs_url" "$rm_cmd_file" "$log_file"
            else
                die "Run somehow neither fastq or pipeline (ini=$ini_name): this should not happen! ($run_info)"
            fi
            echo "  hmf_api_patch -c 'runs' -o '$run_id' -f 'status' -v 'Deleted' -e" >> "$rm_cmd_file"
        done < <(echo "$runs_json" | jq -cr '.[] | [.id,.set.name,.bucket,.status,.ini] | @tsv')
    done < <(echo "$sets_json" | jq -cr '.[] | [.id] | @tsv')

    echo "$processed_ids"
}

process_sample() {
    local sample_id=$1 && shift
    local cmd_file=$1 && shift
    local log_file=$1 && shift
    fastq_json=$(hmf_api_get "fastq?sample_id=$sample_id")

    info "  Processing sample with id $sample_id" >> "$log_file"
    echo "echo '[INFO] Executing steps for sample with id $sample_id'" >> "$rm_cmd_file"

    while read -r fastq; do
        bucket=$(echo "$fastq" | cut -f1)
        name_r1=$(echo "$fastq" | cut -f2)
        name_r2=$(echo "$fastq" | cut -f3)
        bucket=${bucket/_/-} # fix for old API records

        fastq_file_url="gs://$bucket/$name_r1"
        process_gs_url "$sample_status" "$fastq_file_url" "$rm_cmd_file" "$log_file"

        # Existence of R2 FASTQ files depends on sequencer run mode (only with paired end)
        if [[ -n "$name_r2" ]]; then
            fastq_file_url="gs://$bucket/$name_r2"
            process_gs_url "$sample_status" "$fastq_file_url" "$rm_cmd_file" "$log_file"
        fi

    done < <(echo "$fastq_json" | jq -cr '.[] | [.bucket//"",.name_r1//"NA",.name_r2//"NA"] | @tsv')
    # in case bucket is missing check default bucket

    echo "  hmf_api_patch -c 'samples' -o '$sample_id' -f 'status' -v 'Deleted' -e" >> "$cmd_file"
}

run_type_by_ini_name() {
    local ini_name=$1 && shift

    local pipeline="pipeline"
    local fastq="fastq"
    local bcl="bcl"
    local unknown="unknown"

    if [[ "$ini_name" == "BCL.ini" ]]; then
        echo "$bcl"
    elif [[ "$ini_name" == "FastQ.ini" ]]; then
        echo "$fastq"
    elif [[ "$ini_name" == "Somatic.ini" ]]; then
        echo "$pipeline"
    elif [[ "$ini_name" == "SingleSample.ini" ]]; then
        echo "$pipeline"
    elif [[ "$ini_name" == "ShallowSeq.ini" ]]; then
        echo "$pipeline"
    else
        echo "$unknown"
    fi
}

process_gs_url() {
    local status=$1 && shift
    local gs_url=$1 && shift
    local out_file=$1 && shift
    local log_file=$1 && shift
    
    msg=$(gsutil ls "$gs_url" 2>&1)
    if [[ $? -eq 1 ]]; then
        deleted_status="Deleted"
        if [[ "$status" == "$deleted_status" ]]; then
            info "  SKIPPING: no file found at url but object status is $status so makes sense ($gs_url)" >> "$log_file"
        else
            warn "  SKIPPING: no file found at url while object status ($status) is not 'Deleted' ($gs_url)" >> "$log_file" 2>&1
            warn "  SKIPPING: no file found at url while object status ($status) is not 'Deleted' ($gs_url)"
        fi
    else
        info "  OK file found at url so writing rm cmd ($gs_url)" >> "$log_file"
        echo "  gsutil -q $GSUTIL_CMD_PARAMS rm -r $gs_url" >> "$out_file"
    fi
}

function join_by { local IFS="$1"; shift; echo "$*"; }

main
